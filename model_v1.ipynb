{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##ps.pankajsakharkar@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data first\n",
    "\n",
    "import pandas as pd\n",
    "# Load data from the first sheet\n",
    "df = pd.read_excel(\"Artificial_Data.xlsx\", sheet_name=\"Sheet1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ps_project_task\\study\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of the model checkpoint at cardiffnlp/twitter-roberta-base-sentiment-latest were not used when initializing RobertaForSequenceClassification: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "## Function to analyze sentiment\n",
    "from transformers import pipeline\n",
    "model_path = \"cardiffnlp/twitter-roberta-base-sentiment-latest\"\n",
    "sentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path,token=\"\")\n",
    "\n",
    "def get_sentiment_score(text): \n",
    "    \"\"\"This function will return sentiment score of text between 0 -1 \"\"\"\n",
    "    sentiment = sentiment_task(text)\n",
    "    return sentiment[0][\"score\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sample Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "rating_encoder = LabelEncoder()\n",
    "df[\"Rating_encoded\"] = rating_encoder.fit_transform(df[\"Rating\"])\n",
    "\n",
    "df[\"sentiment\"] = df['string_values'].apply(get_sentiment_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from keybert import KeyBERT\n",
    "kw_model = KeyBERT()\n",
    "\n",
    "# Text cleaning function\n",
    "def get_keywords(text):\n",
    "    \"\"\"Function to return impo keywords.\"\"\"\n",
    "    text = text.lower()  # Lowercase\n",
    "    text = re.sub(r\"[^\\w\\s]\", \"\", text)  # Remove punctuation\n",
    "    text = kw_model.extract_keywords(text)\n",
    "    tokens = [i[0] for i in text] # keyword tokens\n",
    "    tokens = [word for word in tokens if word not in stopwords.words(\"english\")]  # Remove stopwords\n",
    "    return \" \".join(tokens) # return in string form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Encode categorical ratings\n",
    "rating_encoder = LabelEncoder()\n",
    "df[\"Rating_encoded\"] = rating_encoder.fit_transform(df[\"Rating\"])\n",
    "\n",
    "# Apply key words extraction\n",
    "df[\"keywords\"] = df[\"string_values\"].astype(str).apply(get_keywords)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Prepared features of text data using TF-IDF techniques"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RATING_TYPE</th>\n",
       "      <th>Rating</th>\n",
       "      <th>FundaIndxint</th>\n",
       "      <th>FundaIndxsga</th>\n",
       "      <th>monthvwretd</th>\n",
       "      <th>monthvwretx</th>\n",
       "      <th>monthewretd</th>\n",
       "      <th>monthewretx</th>\n",
       "      <th>monthsprtrn</th>\n",
       "      <th>monthspindx</th>\n",
       "      <th>...</th>\n",
       "      <th>markets</th>\n",
       "      <th>outlook</th>\n",
       "      <th>positive</th>\n",
       "      <th>reduction</th>\n",
       "      <th>revenue</th>\n",
       "      <th>strategic</th>\n",
       "      <th>supply</th>\n",
       "      <th>taxes</th>\n",
       "      <th>volatility</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Fitch</td>\n",
       "      <td>A+</td>\n",
       "      <td>0.150659</td>\n",
       "      <td>-0.420047</td>\n",
       "      <td>-0.846934</td>\n",
       "      <td>0.560226</td>\n",
       "      <td>0.579303</td>\n",
       "      <td>-0.578314</td>\n",
       "      <td>-0.225006</td>\n",
       "      <td>-0.415867</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Moody's</td>\n",
       "      <td>BB</td>\n",
       "      <td>-0.015715</td>\n",
       "      <td>0.139204</td>\n",
       "      <td>-0.045250</td>\n",
       "      <td>-0.912128</td>\n",
       "      <td>0.497292</td>\n",
       "      <td>-0.910558</td>\n",
       "      <td>1.207592</td>\n",
       "      <td>-0.104050</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fitch</td>\n",
       "      <td>BB</td>\n",
       "      <td>-1.330074</td>\n",
       "      <td>0.001814</td>\n",
       "      <td>-2.564342</td>\n",
       "      <td>0.997115</td>\n",
       "      <td>0.762847</td>\n",
       "      <td>-0.870865</td>\n",
       "      <td>0.230487</td>\n",
       "      <td>0.515273</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Fitch</td>\n",
       "      <td>BBB</td>\n",
       "      <td>-0.025288</td>\n",
       "      <td>0.755935</td>\n",
       "      <td>1.142726</td>\n",
       "      <td>-0.235319</td>\n",
       "      <td>0.637967</td>\n",
       "      <td>-1.611961</td>\n",
       "      <td>0.432309</td>\n",
       "      <td>1.194182</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Moody's</td>\n",
       "      <td>AA</td>\n",
       "      <td>1.343168</td>\n",
       "      <td>1.933543</td>\n",
       "      <td>-1.896747</td>\n",
       "      <td>0.467780</td>\n",
       "      <td>-0.442432</td>\n",
       "      <td>1.349705</td>\n",
       "      <td>0.541002</td>\n",
       "      <td>0.096545</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.447214</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 182 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  RATING_TYPE Rating  FundaIndxint  FundaIndxsga  monthvwretd  monthvwretx  \\\n",
       "0       Fitch     A+      0.150659     -0.420047    -0.846934     0.560226   \n",
       "1     Moody's     BB     -0.015715      0.139204    -0.045250    -0.912128   \n",
       "2       Fitch     BB     -1.330074      0.001814    -2.564342     0.997115   \n",
       "3       Fitch    BBB     -0.025288      0.755935     1.142726    -0.235319   \n",
       "4     Moody's     AA      1.343168      1.933543    -1.896747     0.467780   \n",
       "\n",
       "   monthewretd  monthewretx  monthsprtrn  monthspindx  ...  markets  outlook  \\\n",
       "0     0.579303    -0.578314    -0.225006    -0.415867  ...      0.0      0.0   \n",
       "1     0.497292    -0.910558     1.207592    -0.104050  ...      0.0      0.0   \n",
       "2     0.762847    -0.870865     0.230487     0.515273  ...      0.0      0.0   \n",
       "3     0.637967    -1.611961     0.432309     1.194182  ...      0.0      0.0   \n",
       "4    -0.442432     1.349705     0.541002     0.096545  ...      0.0      0.0   \n",
       "\n",
       "   positive  reduction   revenue  strategic    supply  taxes  volatility  year  \n",
       "0       0.0        0.0  0.000000   0.447214  0.447214    0.0         0.0   0.0  \n",
       "1       0.0        0.0  0.447214   0.000000  0.000000    0.0         0.0   0.0  \n",
       "2       0.0        0.0  0.447214   0.000000  0.000000    0.0         0.0   0.0  \n",
       "3       0.0        0.0  0.447214   0.000000  0.000000    0.0         0.0   0.0  \n",
       "4       0.0        0.0  0.447214   0.000000  0.000000    0.0         0.0   0.0  \n",
       "\n",
       "[5 rows x 182 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Convert text into TF-IDF features\n",
    "tfidf_vectorizer = TfidfVectorizer(max_features=100)\n",
    "tfidf_features = tfidf_vectorizer.fit_transform(df[\"keywords\"])\n",
    "\n",
    "# Convert to DataFrame\n",
    "tfidf_df = pd.DataFrame(tfidf_features.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n",
    "\n",
    "# Concatenate with original dataset\n",
    "df_combined = pd.concat([df, tfidf_df], axis=1)\n",
    "\n",
    "df_combined.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Data spliting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "X_structured = df_combined.drop(columns=[\"Rating\", \"RATING_TYPE\", \"string_values\", \"keywords\", \"Rating_encoded\"])\n",
    "X_text = tfidf_df                   # Text-only features\n",
    "y = df_combined[\"Rating_encoded\"]   # Target variable\n",
    "\n",
    "# Spliting data \n",
    "X_train_struct, X_test_struct, y_train, y_test = train_test_split(X_structured, y, test_size=0.2, random_state=42)\n",
    "X_train_text, X_test_text, _, _ = train_test_split(X_text, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Now I wil build three models -\n",
    "# 1 - without string data (structured data only)\n",
    "# 2 - with string type TFIDF data\n",
    "# 3 - with combined data \n",
    "\n",
    "# then build models and check predictions and metrics (classification report, model accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ps_project_task\\study\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\ps_project_task\\study\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\ps_project_task\\study\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\ps_project_task\\study\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\ps_project_task\\study\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\ps_project_task\\study\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\ps_project_task\\study\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\ps_project_task\\study\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\ps_project_task\\study\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define target and feature sets\n",
    "X_structured = df_combined.drop(columns=[\"Rating\", \"RATING_TYPE\", \"string_values\", \"keywords\", \"Rating_encoded\"])\n",
    "X_text = tfidf_df  # Text-only features\n",
    "y = df_combined[\"Rating_encoded\"]  # Target variable\n",
    "\n",
    "# Split data \n",
    "X_train_struct, X_test_struct, y_train, y_test = train_test_split(X_structured, y, test_size=0.2, random_state=42)\n",
    "X_train_text, X_test_text, _, _ = train_test_split(X_text, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train models\n",
    "model_structured = DecisionTreeClassifier( random_state=42)\n",
    "model_text = DecisionTreeClassifier( random_state=42)\n",
    "model_combined = DecisionTreeClassifier( random_state=42)\n",
    "\n",
    "model_structured.fit(X_train_struct, y_train)\n",
    "model_text.fit(X_train_text, y_train)\n",
    "model_combined.fit(pd.concat([X_train_struct, X_train_text], axis=1), y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_struct = model_structured.predict(X_test_struct)\n",
    "y_pred_text = model_text.predict(X_test_text)\n",
    "y_pred_combined = model_combined.predict(pd.concat([X_test_struct, X_test_text], axis=1))\n",
    "\n",
    "# Evaluate performance\n",
    "report_struct = classification_report(y_test, y_pred_struct)\n",
    "report_text = classification_report(y_test, y_pred_text)\n",
    "report_combined = classification_report(y_test, y_pred_combined)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      0.67      0.44         3\n",
      "           1       0.00      0.00      0.00         3\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00         4\n",
      "           7       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.10        20\n",
      "   macro avg       0.04      0.08      0.06        20\n",
      "weighted avg       0.05      0.10      0.07        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.00      0.00      0.00         3\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.10      0.25      0.14         4\n",
      "           7       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.05        20\n",
      "   macro avg       0.01      0.03      0.02        20\n",
      "weighted avg       0.02      0.05      0.03        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.50      0.67      0.57         3\n",
      "           1       0.50      0.33      0.40         3\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.25      0.25      0.25         4\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00         4\n",
      "           7       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.20        20\n",
      "   macro avg       0.16      0.16      0.15        20\n",
      "weighted avg       0.20      0.20      0.20        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report_combined)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion for decision Tree model : \n",
    "- model-3 accuracy is 20%\n",
    "- here data volumne is very low for model building.\n",
    "- If we could have mode text data , we can extract more important keywords and do sentiment analysis techniques.\n",
    "- model 3 accuracy is high as comapre to other models , this model is combination of sentiment score feature, extracted keywords featurs with original structured data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\ps_project_task\\study\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\ps_project_task\\study\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\ps_project_task\\study\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\ps_project_task\\study\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\ps_project_task\\study\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\ps_project_task\\study\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\ps_project_task\\study\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\ps_project_task\\study\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "e:\\ps_project_task\\study\\venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Define target and feature sets\n",
    "X_structured = df_combined.drop(columns=[\"Rating\", \"RATING_TYPE\", \"string_values\", \"keywords\", \"Rating_encoded\"])\n",
    "X_text = tfidf_df  # Text-only features\n",
    "y = df_combined[\"Rating_encoded\"]  # Target variable\n",
    "\n",
    "# Split data \n",
    "X_train_struct, X_test_struct, y_train, y_test = train_test_split(X_structured, y, test_size=0.2, random_state=42)\n",
    "X_train_text, X_test_text, _, _ = train_test_split(X_text, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train models\n",
    "model_structured = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "model_text = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "model_combined = RandomForestClassifier(n_estimators=50, random_state=42)\n",
    "\n",
    "model_structured.fit(X_train_struct, y_train)\n",
    "model_text.fit(X_train_text, y_train)\n",
    "model_combined.fit(pd.concat([X_train_struct, X_train_text], axis=1), y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_struct = model_structured.predict(X_test_struct)\n",
    "y_pred_text = model_text.predict(X_test_text)\n",
    "y_pred_combined = model_combined.predict(pd.concat([X_test_struct, X_test_text], axis=1))\n",
    "\n",
    "# Evaluate performance\n",
    "report_struct = classification_report(y_test, y_pred_struct)\n",
    "report_text = classification_report(y_test, y_pred_text)\n",
    "report_combined = classification_report(y_test, y_pred_combined)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.33      0.50         3\n",
      "           1       0.12      0.33      0.18         3\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.00      0.00      0.00         4\n",
      "           7       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.10        20\n",
      "   macro avg       0.14      0.08      0.09        20\n",
      "weighted avg       0.17      0.10      0.10        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report_struct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.00      0.00      0.00         3\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.10      0.25      0.14         4\n",
      "           7       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.05        20\n",
      "   macro avg       0.01      0.03      0.02        20\n",
      "weighted avg       0.02      0.05      0.03        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       0.00      0.00      0.00         3\n",
      "           2       0.00      0.00      0.00         2\n",
      "           3       0.00      0.00      0.00         4\n",
      "           4       0.00      0.00      0.00         1\n",
      "           5       0.00      0.00      0.00         1\n",
      "           6       0.12      0.25      0.17         4\n",
      "           7       0.00      0.00      0.00         2\n",
      "\n",
      "    accuracy                           0.05        20\n",
      "   macro avg       0.02      0.03      0.02        20\n",
      "weighted avg       0.03      0.05      0.03        20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(report_combined)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Conclusion for Random forest model : \n",
    "- model-1 accuracy is 10%\n",
    "- here data volumne is very low for model building.\n",
    "- If we could have mode text data , we can extract more important keywords and do sentiment analysis techniques.\n",
    "- model 3 accuracy is high as comapre to other models , this model is combination of sentiment score feature, extracted keywords featurs with original structured data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
